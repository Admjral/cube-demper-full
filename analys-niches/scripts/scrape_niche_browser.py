"""
–°–∫—Ä–µ–π–ø–µ—Ä –¥–∞–Ω–Ω—ã—Ö –∞–Ω–∞–ª–∏—Ç–∏–∫–∏ –Ω–∏—à —á–µ—Ä–µ–∑ –±—Ä–∞—É–∑–µ—Ä

–ò—Å–ø–æ–ª—å–∑—É–µ—Ç Playwright –¥–ª—è:
1. –ê–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏
2. –ù–∞–≤–∏–≥–∞—Ü–∏–∏ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
3. –°–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –æ —Ç–æ–≤–∞—Ä–∞—Ö
4. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ CSV
"""

import asyncio
import csv
import json
import os
import re
from datetime import datetime
from pathlib import Path
from typing import Optional

from dotenv import load_dotenv
from playwright.async_api import async_playwright, Page, Browser

# –ó–∞–≥—Ä—É–∑–∫–∞ credentials
load_dotenv()

EMAIL = os.getenv("NICHE_SCRAPER_EMAIL")
PASSWORD = os.getenv("NICHE_SCRAPER_PASSWORD")
SITE_URL = os.getenv("NICHE_SCRAPER_SITE", "https://app.algatop.kz")
NICHE_URL = os.getenv("NICHE_SCRAPER_NICHE", "https://app.algatop.kz/niche")

# –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö
DATA_DIR = Path(__file__).parent.parent / "data"
DATA_DIR.mkdir(exist_ok=True)


class NicheDataScraper:
    def __init__(self, headless: bool = False):
        self.headless = headless
        self.browser: Optional[Browser] = None
        self.page: Optional[Page] = None
        self.products = []
        self.categories = []

    async def start(self):
        """–ó–∞–ø—É—Å–∫ –±—Ä–∞—É–∑–µ—Ä–∞"""
        playwright = await async_playwright().start()
        self.browser = await playwright.chromium.launch(
            headless=self.headless,
            slow_mo=100  # –ó–∞–º–µ–¥–ª–µ–Ω–∏–µ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        )
        self.page = await self.browser.new_page()

        # –£–≤–µ–ª–∏—á–µ–Ω–Ω—ã–µ —Ç–∞–π–º–∞—É—Ç—ã
        self.page.set_default_timeout(60000)

        print("üöÄ –ë—Ä–∞—É–∑–µ—Ä –∑–∞–ø—É—â–µ–Ω")

    async def login(self):
        """–ê–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –Ω–∞ —Å–∞–π—Ç–µ"""
        print(f"üîê –ê–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –∫–∞–∫ {EMAIL}...")

        # –ü–µ—Ä–µ—Ö–æ–¥ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É –≤—Ö–æ–¥–∞
        await self.page.goto(SITE_URL)
        await self.page.wait_for_load_state("networkidle")

        # –ò—â–µ–º –∫–Ω–æ–ø–∫—É –≤—Ö–æ–¥–∞ –∏–ª–∏ —Ñ–æ—Ä–º—É
        # –ê–¥–∞–ø—Ç–∏—Ä—É–π —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –ø–æ–¥ —Ä–µ–∞–ª—å–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Å–∞–π—Ç–∞
        try:
            # –ü–æ–ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –∫–Ω–æ–ø–∫—É "–í–æ–π—Ç–∏"
            login_btn = await self.page.query_selector('text=–í–æ–π—Ç–∏')
            if login_btn:
                await login_btn.click()
                await self.page.wait_for_load_state("networkidle")
        except:
            pass

        # –ó–∞–ø–æ–ª–Ω—è–µ–º —Ñ–æ—Ä–º—É
        # –≠—Ç–∏ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –Ω—É–∂–Ω–æ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ–¥ —Ä–µ–∞–ª—å–Ω—ã–π —Å–∞–π—Ç
        email_input = await self.page.query_selector('input[type="email"], input[name="email"], input[placeholder*="mail"]')
        if email_input:
            await email_input.fill(EMAIL)

        password_input = await self.page.query_selector('input[type="password"]')
        if password_input:
            await password_input.fill(PASSWORD)

        # –ö–ª–∏–∫ –Ω–∞ –∫–Ω–æ–ø–∫—É –≤—Ö–æ–¥–∞
        submit_btn = await self.page.query_selector('button[type="submit"], button:has-text("–í–æ–π—Ç–∏")')
        if submit_btn:
            await submit_btn.click()
            await self.page.wait_for_load_state("networkidle")

        # –ñ–¥—ë–º —Ä–µ–¥–∏—Ä–µ–∫—Ç–∞
        await asyncio.sleep(3)
        print(f"‚úÖ –ê–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞. URL: {self.page.url}")

    async def go_to_niche_page(self):
        """–ü–µ—Ä–µ—Ö–æ–¥ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É –ø–æ–∏—Å–∫–∞ –Ω–∏—à"""
        print(f"üìä –ü–µ—Ä–µ—Ö–æ–¥ –Ω–∞ {NICHE_URL}...")
        await self.page.goto(NICHE_URL)
        await self.page.wait_for_load_state("networkidle")
        await asyncio.sleep(2)
        print(f"‚úÖ –ù–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ –Ω–∏—à. URL: {self.page.url}")

    async def get_categories(self) -> list:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–π"""
        print("üìÇ –ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–π...")

        # –ò—â–µ–º —ç–ª–µ–º–µ–Ω—Ç—ã –∫–∞—Ç–µ–≥–æ—Ä–∏–π
        # –ê–¥–∞–ø—Ç–∏—Ä—É–π —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –ø–æ–¥ —Ä–µ–∞–ª—å–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É
        category_elements = await self.page.query_selector_all(
            '[class*="category"], [class*="niche"], li[data-category], .sidebar a, .menu a'
        )

        categories = []
        for el in category_elements:
            text = await el.text_content()
            href = await el.get_attribute("href")
            if text and text.strip():
                categories.append({
                    "name": text.strip(),
                    "url": href
                })

        print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(categories)} –∫–∞—Ç–µ–≥–æ—Ä–∏–π")
        self.categories = categories
        return categories

    async def scrape_products_from_table(self) -> list:
        """–°–∫—Ä–µ–π–ø–∏–Ω–≥ —Ç–æ–≤–∞—Ä–æ–≤ –∏–∑ —Ç–∞–±–ª–∏—Ü—ã –Ω–∞ —Ç–µ–∫—É—â–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ"""
        products = []

        # –ñ–¥—ë–º –∑–∞–≥—Ä—É–∑–∫–∏ —Ç–∞–±–ª–∏—Ü—ã
        await self.page.wait_for_selector('table, [class*="table"], [class*="grid"]', timeout=10000)

        # –ò—â–µ–º —Å—Ç—Ä–æ–∫–∏ —Ç–∞–±–ª–∏—Ü—ã
        rows = await self.page.query_selector_all('table tbody tr, [class*="row"], [class*="item"]')

        for row in rows:
            try:
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ —è—á–µ–µ–∫
                # –ê–¥–∞–ø—Ç–∏—Ä—É–π —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –ø–æ–¥ —Ä–µ–∞–ª—å–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Ç–∞–±–ª–∏—Ü—ã Algatop
                cells = await row.query_selector_all('td, [class*="cell"], [class*="col"]')

                if len(cells) >= 4:
                    product = {
                        "name": await self._get_text(cells[0]) if len(cells) > 0 else "",
                        "sales": await self._get_number(cells[1]) if len(cells) > 1 else 0,
                        "rating": await self._get_float(cells[2]) if len(cells) > 2 else 0.0,
                        "reviews": await self._get_number(cells[3]) if len(cells) > 3 else 0,
                        "sellers": await self._get_number(cells[4]) if len(cells) > 4 else 0,
                        "revenue": await self._get_number(cells[5]) if len(cells) > 5 else 0,
                        "scraped_at": datetime.now().isoformat()
                    }

                    if product["name"]:  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
                        products.append(product)

            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å—Ç—Ä–æ–∫–∏: {e}")
                continue

        return products

    async def _get_text(self, element) -> str:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ —ç–ª–µ–º–µ–Ω—Ç–∞"""
        if element:
            text = await element.text_content()
            return text.strip() if text else ""
        return ""

    async def _get_number(self, element) -> int:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —á–∏—Å–ª–∞ –∏–∑ —ç–ª–µ–º–µ–Ω—Ç–∞"""
        text = await self._get_text(element)
        # –£–±–∏—Ä–∞–µ–º –ø—Ä–æ–±–µ–ª—ã –∏ –Ω–µ—á–∏—Å–ª–æ–≤—ã–µ —Å–∏–º–≤–æ–ª—ã
        numbers = re.sub(r'[^\d]', '', text)
        return int(numbers) if numbers else 0

    async def _get_float(self, element) -> float:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ float –∏–∑ —ç–ª–µ–º–µ–Ω—Ç–∞"""
        text = await self._get_text(element)
        # –ó–∞–º–µ–Ω—è–µ–º –∑–∞–ø—è—Ç—É—é –Ω–∞ —Ç–æ—á–∫—É
        text = text.replace(',', '.')
        numbers = re.findall(r'[\d.]+', text)
        return float(numbers[0]) if numbers else 0.0

    async def scroll_and_load_all(self, max_scrolls: int = 50):
        """–ü—Ä–æ–∫—Ä—É—Ç–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö"""
        print("üìú –ü—Ä–æ–∫—Ä—É—Ç–∫–∞ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö...")

        last_height = 0
        scrolls = 0

        while scrolls < max_scrolls:
            # –ü—Ä–æ–∫—Ä—É—Ç–∫–∞ –≤–Ω–∏–∑
            await self.page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
            await asyncio.sleep(1)

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∏–∑–º–µ–Ω–∏–ª–∞—Å—å –ª–∏ –≤—ã—Å–æ—Ç–∞
            new_height = await self.page.evaluate("document.body.scrollHeight")
            if new_height == last_height:
                break

            last_height = new_height
            scrolls += 1

            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å
            if scrolls % 10 == 0:
                print(f"  ... –ø—Ä–æ–∫—Ä—É—Ç–∫–∞ {scrolls}/{max_scrolls}")

        print(f"‚úÖ –ü—Ä–æ–∫—Ä—É—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ ({scrolls} —Å–∫—Ä–æ–ª–ª–æ–≤)")

    async def scrape_category(self, category_url: str, category_name: str) -> list:
        """–°–∫—Ä–µ–π–ø–∏–Ω–≥ –æ–¥–Ω–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏"""
        print(f"\nüìÅ –°–∫—Ä–µ–π–ø–∏–Ω–≥ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏: {category_name}")

        await self.page.goto(category_url)
        await self.page.wait_for_load_state("networkidle")
        await asyncio.sleep(2)

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ —á–µ—Ä–µ–∑ —Å–∫—Ä–æ–ª–ª
        await self.scroll_and_load_all()

        # –ü–∞—Ä—Å–∏–º —Ç–∞–±–ª–∏—Ü—É
        products = await self.scrape_products_from_table()

        # –î–æ–±–∞–≤–ª—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é –∫ –∫–∞–∂–¥–æ–º—É —Ç–æ–≤–∞—Ä—É
        for product in products:
            product["category"] = category_name

        print(f"‚úÖ –°–æ–±—Ä–∞–Ω–æ {len(products)} —Ç–æ–≤–∞—Ä–æ–≤ –∏–∑ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ {category_name}")
        return products

    async def save_to_csv(self, filename: str = None):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ CSV"""
        if not filename:
            filename = f"algatop_export_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"

        filepath = DATA_DIR / filename

        if not self.products:
            print("‚ö†Ô∏è –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è")
            return

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ –∏–∑ –ø–µ—Ä–≤–æ–≥–æ –ø—Ä–æ–¥—É–∫—Ç–∞
        headers = list(self.products[0].keys())

        with open(filepath, 'w', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=headers)
            writer.writeheader()
            writer.writerows(self.products)

        print(f"\nüíæ –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {filepath}")
        print(f"   –í—Å–µ–≥–æ —Ç–æ–≤–∞—Ä–æ–≤: {len(self.products)}")

    async def save_to_json(self, filename: str = None):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ JSON"""
        if not filename:
            filename = f"algatop_export_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"

        filepath = DATA_DIR / filename

        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump({
                "scraped_at": datetime.now().isoformat(),
                "total_products": len(self.products),
                "categories": self.categories,
                "products": self.products
            }, f, ensure_ascii=False, indent=2)

        print(f"üíæ JSON —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ {filepath}")

    async def close(self):
        """–ó–∞–∫—Ä—ã—Ç–∏–µ –±—Ä–∞—É–∑–µ—Ä–∞"""
        if self.browser:
            await self.browser.close()
            print("üîí –ë—Ä–∞—É–∑–µ—Ä –∑–∞–∫—Ä—ã—Ç")

    async def debug_page_structure(self):
        """–û—Ç–ª–∞–¥–∫–∞: –ø–æ–∫–∞–∑–∞—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        print("\nüîç DEBUG: –°—Ç—Ä—É–∫—Ç—É—Ä–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã")
        print(f"URL: {self.page.url}")
        print(f"Title: {await self.page.title()}")

        # –°–∫—Ä–∏–Ω—à–æ—Ç
        screenshot_path = DATA_DIR / "debug_screenshot.png"
        await self.page.screenshot(path=screenshot_path, full_page=True)
        print(f"üì∏ –°–∫—Ä–∏–Ω—à–æ—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {screenshot_path}")

        # HTML —Å—Ç—Ä—É–∫—Ç—É—Ä–∞
        html_path = DATA_DIR / "debug_page.html"
        html = await self.page.content()
        with open(html_path, 'w', encoding='utf-8') as f:
            f.write(html)
        print(f"üìÑ HTML —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {html_path}")


async def main():
    """–û—Å–Ω–æ–≤–Ω–æ–π —Å–∫—Ä–∏–ø—Ç"""
    print("=" * 60)
    print("ALGATOP SCRAPER")
    print("=" * 60)

    if not EMAIL or not PASSWORD:
        print("‚ùå –û—à–∏–±–∫–∞: –ù–µ –∑–∞–¥–∞–Ω—ã ALGATOP_EMAIL –∏ ALGATOP_PASSWORD –≤ .env")
        return

    scraper = NicheDataScraper(headless=False)  # headless=False –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏

    try:
        await scraper.start()
        await scraper.login()
        await scraper.go_to_niche_page()

        # –°–Ω–∞—á–∞–ª–∞ –ø–æ—Å–º–æ—Ç—Ä–∏–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        await scraper.debug_page_structure()

        # –ü–æ–ø—Ä–æ–±—É–µ–º —Å–ø–∞—Ä—Å–∏—Ç—å —Ç–µ–∫—É—â—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
        products = await scraper.scrape_products_from_table()
        scraper.products.extend(products)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º
        await scraper.save_to_csv()
        await scraper.save_to_json()

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–∫—Ä–∏–Ω—à–æ—Ç –æ—à–∏–±–∫–∏
        if scraper.page:
            await scraper.page.screenshot(path=DATA_DIR / "error_screenshot.png")

    finally:
        await scraper.close()


if __name__ == "__main__":
    asyncio.run(main())
